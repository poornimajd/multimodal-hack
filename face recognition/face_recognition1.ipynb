{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqtftuICxDte"
   },
   "outputs": [],
   "source": [
    "########to load landmarks for face###############\n",
    "import bz2\n",
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def download_landmarks(dst_file):\n",
    "    url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    with urlopen(url) as src, open(dst_file, 'wb') as dst:\n",
    "        data = src.read(1024)\n",
    "        while len(data) > 0:\n",
    "            dst.write(decompressor.decompress(data))\n",
    "            data = src.read(1024)\n",
    "\n",
    "dst_dir = 'F:/face2/face-recognition-master/models'\n",
    "dst_file = os.path.join(dst_dir, 'landmarks.dat')\n",
    "\n",
    "if not os.path.exists(dst_file):\n",
    "    os.makedirs(dst_dir)\n",
    "    download_landmarks(dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11036,
     "status": "error",
     "timestamp": 1555656546275,
     "user": {
      "displayName": "Sarvesh Angadi",
      "photoUrl": "https://lh5.googleusercontent.com/-n_62dRHelE4/AAAAAAAAAAI/AAAAAAAAALw/JjEJ1Y5M9Jg/s64/photo.jpg",
      "userId": "10256897694498453208"
     },
     "user_tz": -330
    },
    "id": "jPYNdCKixDtj",
    "outputId": "ea957397-4874-4898-cb7c-3ee182950bbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#########creating model ############\n",
    "from model import create_model\n",
    "\n",
    "nn4_small2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2140,
     "status": "error",
     "timestamp": 1555648637675,
     "user": {
      "displayName": "Sarvesh Angadi",
      "photoUrl": "https://lh5.googleusercontent.com/-n_62dRHelE4/AAAAAAAAAAI/AAAAAAAAALw/JjEJ1Y5M9Jg/s64/photo.jpg",
      "userId": "10256897694498453208"
     },
     "user_tz": -330
    },
    "id": "gvyDN1LHxDtr",
    "outputId": "a5bebee4-4421-4f1f-fe79-ab0fb6d4384a"
   },
   "outputs": [],
   "source": [
    "############triplet loss computation and add it as a layer to the model nn4###############\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(96, 96, 3))\n",
    "in_p = Input(shape=(96, 96, 3))\n",
    "in_n = Input(shape=(96, 96, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqrBSNCSxDtv"
   },
   "outputs": [],
   "source": [
    "########using the nn4 pretrained model,its basically a inception model##########\n",
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('F:/face2/face-recognition-master/weights/nn4.small2.v1.h5')#load the weights for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJzUiAX-xDt1"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #return img[...,::-1]\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('F:/face2/face-recognition-master/models/landmarks.dat') ###for aligning the face using dlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k49kC0-cxDt5"
   },
   "outputs": [],
   "source": [
    "###return aligned image\n",
    "def align_image(img):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############run from here#####################################################################\n",
    "#######################################################3\n",
    "##################################333321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['stupid']\n",
      "['stupid', 'swine']\n"
     ]
    }
   ],
   "source": [
    "#########take the live feed input and also to extract embeddings from the live feed of training image########\n",
    "from tkinter import *\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tkinter\n",
    "#window=tkinter.Tk()\n",
    "good_image_indx = []\n",
    "unfit_image_indx = []\n",
    "\n",
    "count=0\n",
    "cc=0\n",
    "\n",
    "user=[]\n",
    "embedded = np.zeros((100, 128)) ####the shape is 100,.. beacuse 10 images for one person so lenght needed is 10 for one person ,like 128 embeddings of each person will be extracted from the 10 images of the same person i.e 10 frames of the vedio,so here 100 meaning we can feed 10 people at a time and extract their embeddings and store it into this array\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    imgf = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if cc==10:###wen the 10 images or 10 frames of one person is done then the vedio wil stop for a delay of 15 secs,i.e it wont extract any embeddings from the vedio,and after 15secs the text appears to change the person,and the next person shuld appear in front of camera and then after 10 frames again the vedio will pause and this can continue for how many ever people u need ,but the embeddings can be stored only for the first 10 people\n",
    "        cv2.putText(img = imgf, text = \"change\", org = (21,25), fontFace = cv2.FONT_HERSHEY_DUPLEX, fontScale = 1, \n",
    "                        color = (0, 255, 0))\n",
    "        def show_entry_fields():\n",
    "            print(\"First Name: %s\\nLast Name: %s\" % (e1.get(), e2.get()))\n",
    "\n",
    "        master = tkinter.Tk()\n",
    "        master.geometry('360x100')\n",
    "        def whenpressed():\n",
    "            user.append(e1.get())\n",
    "            master.destroy()\n",
    "        master.title(\"Face_Recognition\")\n",
    "        Label(master, text=\"Please enter your name and press quit\").grid(row=3,column=1)\n",
    "        Label(master, text=\"Face Recognition\", fg = \"white\",bg = \"black\",font = \"Helvetica 14 bold italic\").grid(row=2,column=1)\n",
    "\n",
    "        e1 = Entry(master)\n",
    "        #e2 = Entry(master)\n",
    "\n",
    "        e1.grid(row=4, column=1)\n",
    "        #e2.grid(row=1, column=1)\n",
    "        print(user)\n",
    "        #print(e1.get())\n",
    "        Button(master, text='Quit', width=50,command=whenpressed).grid(row=5, column=1, sticky=W, pady=4)\n",
    "        #Button(master, text='Show', command=show_entry_fields).grid(row=3, column=1, sticky=W, pady=4)\n",
    "\n",
    "        mainloop( )\n",
    "\n",
    "        #res=Label(master)\n",
    "        cv2.putText(img = imgf, text = \"changexbro\", org = (45,48), fontFace = cv2.FONT_HERSHEY_DUPLEX, fontScale = 1, \n",
    "                        color = (0, 255, 0)) ###this line is redundant\n",
    "        key = cv2.waitKey(10000)\n",
    "        cc=0 ##here cc is again made 0 for the next person , till cc reaches 9 the below whole process of embedding extraction keeps happening for all the 10 frames of the person\n",
    "    # Capture frame-by-frame\n",
    "\n",
    "    img = align_image(imgf) ##call the align image function for extracting the aligned face for the person\n",
    "    try:\n",
    "        img = (img / 255.).astype(np.float32)\n",
    "        embedded[count] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]### here embeddings are stored for all the people ,first 10 are for the first person ,next 10 for the next person and so on\n",
    "        cc=cc+1###here cc is incremented , it is incremented only wen the embedding is extracted ,nt wen it is not clear to extract,this is done to have only 10 proper embeddings and not the error\n",
    "        count+=1## this is never made 0 , it is for incremneting for all the people like if it is 4 peopel it wil be incremented till 40\n",
    "        good_image_indx.append(count) ###redundant \n",
    "    except TypeError:\n",
    "        print(\"The image is not Clear to extract the Embeddings\")\n",
    "\n",
    "    # Display the resulting frame  #####is done wen the embeddings are not clear\n",
    "        unfit_image_indx.append(count)\n",
    "        #count+=1\n",
    "    cv2.imshow('frame',imgf) ######this frame only does all the capturing of 10 frames of each person and then stopping for 15 secs and again starting\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'swine', '']\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(user)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5K0JWx7rxDuK",
    "outputId": "db9e5fdd-9d71-4644-bf95-9c31f877545c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'stupid', 'swine', 'swine', 'swine', 'swine', 'swine', 'swine', 'swine', 'swine', 'swine', 'swine', '', '', '', '', '', '', '', '', '', '']\n",
      "KNN accuracy = 0.6666666666666666, SVM accuracy = 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "#########this is for training of knn and svm \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "a=[]\n",
    "\n",
    "for jj in range(int(count/10)): ###### the count keeps varying depending on the number of people we live feed it,so here we just divide it  by 10 to get the number of people ,it is divided by 10 because we take 10 images or frames of one person\n",
    "     a.extend([user[jj] for x in range(10)]) ###this is like having labels for the persons, so first 10 images or first 10 elements of array a will be given name as '0' because the first 10 images are of one personm only ,the next 10 elemnts of a will be given name as '1' beacuse its like the label for the next person \n",
    "\n",
    "print(a)\n",
    "targets = np.array(a)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)#######the labels\n",
    "embedded=embedded[0:((int(count/10))*10)] #######this is done beacuse the lenght of embedded can be anything between 0 to 100,depending on the number of people taken during the live feed,so to determine hoe much shuld the lenght be this computstion is done , because the lengt is not a constant\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange((int(count/10))*10) % 2 != 0 ###the same reason is here ,we can determine the lenght beforehand ,since it varies dynamically,%2 is just inbuilt\n",
    "test_idx = np.arange((int(count/10))*10) % 2 == 0\n",
    "\n",
    "#######the below code is just for training the ml classifiers\n",
    "X_train = embedded[train_idx]\n",
    "\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = LinearSVC()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print('KNN accuracy = {0}, SVM accuracy = {1}'.format(acc_knn,acc_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiedA_mZxDug"
   },
   "outputs": [],
   "source": [
    "\n",
    "########a function for the perprocessing for the testing part \n",
    "def lol(img):\n",
    "        embedded1 = np.zeros((100, 128))\n",
    "        img = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = align_image(img)\n",
    "        try:\n",
    "            img = (img / 255.).astype(np.float32)\n",
    "        except TypeError:\n",
    "\n",
    "            print(\"The image is not Clear to extract the Embeddings\")\n",
    "        else:\n",
    "            embedded1 = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
    "        testing= np.arange(100)\n",
    "        example_prediction = knn.predict([embedded1])\n",
    "        example_identity = encoder.inverse_transform(example_prediction)[0]\n",
    "        return example_identity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zap84cPPxDuj",
    "outputId": "9ccf2222-5678-4024-d359-5fcf09963cec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is not Clear to extract the Embeddings\n",
      "improper angle or no face\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n",
      "The image is not Clear to extract the Embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from align import AlignDlib\n",
    "cap = cv2.VideoCapture(0)\n",
    "#test = load_metadata('F:/face2/face-recognition-master/testimages/')\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') \n",
    "out = cv2.VideoWriter('F:/face2/face-recognition-master/outputlastnt.avi', fourcc, 20.0, (640, 480)) #not so imp\n",
    "\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    #cv2.imshow('framezs',frame)\n",
    "    # Initialize the OpenFace face alignment utility\n",
    "    try:\n",
    "\n",
    "\n",
    "        # Detect face and return bounding box\n",
    "        bb = alignment.getLargestFaceBoundingBox(frame)\n",
    "        try:\n",
    "            cv2.rectangle(frame,(bb.left(), bb.top()+bb.height()),(bb.left()+bb.width(), bb.top()),(0,255,0),3)\n",
    "            #cv2.imshow('framgz',frame) ####the bounding box is put around the face and the coordinates are got from the alignment h    of \n",
    "        except:\n",
    "            #cv2.imshow('framhz',frame)\n",
    "            out.write(frame)\n",
    "            print(\"improper angle or no face\")\n",
    "        # Our operations on the frame come here\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        example_identity = lol(frame)\n",
    "        ###########\n",
    "\n",
    "\n",
    "        cv2.putText(img = frame, text = \"{}\".format(example_identity), org = (bb.left(), bb.top()+bb.height()), fontFace = cv2.FONT_HERSHEY_DUPLEX, fontScale = 1, \n",
    "                        color = (255, 0, 0))\n",
    "\n",
    "        #frame = cv2.flip(frame,0)\n",
    "        ###################\n",
    "        out.write(frame)\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frameq',frame) \n",
    "    except:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "        #cv2.imshow('framez',frame) q\n",
    "    #print(('Recogn03zed as {}'.format(example_identity)))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZZq_9qlxDuq"
   },
   "outputs": [],
   "source": [
    "#########for all faces#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "face_recognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
